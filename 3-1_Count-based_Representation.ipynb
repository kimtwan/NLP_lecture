{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3-1. **Count-based Representation**"
      ],
      "metadata": {
        "id": "QBX084k_RY2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk"
      ],
      "metadata": {
        "id": "XtspuIH4pUx3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords as sw\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd5k7yOzFrmc",
        "outputId": "7dccffa7-ae05-47f6-de95-b14089472e17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'Kim loves the NLP. The NLP hates Kim', # document 0\n",
        "    'Kim hates the NLP' # document 1\n",
        "]"
      ],
      "metadata": {
        "id": "iIDiin57TKpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-process the documents"
      ],
      "metadata": {
        "id": "b-dWVBvshQdR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcjJPxPIHsVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5520e55c-f354-4c39-f385-e4e0817998ea"
      },
      "source": [
        "stop_words = sw.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokenized_docs = []\n",
        "for doc_id, doc in enumerate(corpus):\n",
        "    print('doc_id:', doc_id)\n",
        "    # remove punctuations\n",
        "    puct_removed = re.sub(r'[^\\w\\s]','',doc)\n",
        "    print('punctuations removed:', puct_removed)\n",
        "    # tokenize words\n",
        "    tokenized = word_tokenize(puct_removed)\n",
        "    print('tokenized:', tokenized)\n",
        "    # case folding\n",
        "    lowered = [t.lower() for t in tokenized]\n",
        "    print('case-folded:', lowered)\n",
        "    # lemmatization\n",
        "    lemmatized = [lemmatizer.lemmatize(t) for t in lowered]\n",
        "    print('lemmatized:', lemmatized)\n",
        "    # stop word removal\n",
        "    tokenized_doc = [w for w in lemmatized if not w in stop_words]\n",
        "    print('stopwords removed:', tokenized_doc)\n",
        "    tokenized_docs.append(tokenized_doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc_id: 0\n",
            "punctuations removed: Kim loves the NLP The NLP hates Kim\n",
            "tokenized: ['Kim', 'loves', 'the', 'NLP', 'The', 'NLP', 'hates', 'Kim']\n",
            "case-folded: ['kim', 'loves', 'the', 'nlp', 'the', 'nlp', 'hates', 'kim']\n",
            "lemmatized: ['kim', 'love', 'the', 'nlp', 'the', 'nlp', 'hate', 'kim']\n",
            "stopwords removed: ['kim', 'love', 'nlp', 'nlp', 'hate', 'kim']\n",
            "doc_id: 1\n",
            "punctuations removed: Kim hates the NLP\n",
            "tokenized: ['Kim', 'hates', 'the', 'NLP']\n",
            "case-folded: ['kim', 'hates', 'the', 'nlp']\n",
            "lemmatized: ['kim', 'hate', 'the', 'nlp']\n",
            "stopwords removed: ['kim', 'hate', 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDHm9lxmkMeG",
        "outputId": "2a2fbf0b-82c3-447b-db85-faab40021649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['kim', 'love', 'nlp', 'nlp', 'hate', 'kim'], ['kim', 'hate', 'nlp']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot encoding"
      ],
      "metadata": {
        "id": "Ih1ngdS8ovvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_set = set()\n",
        "for tokenized_doc in tokenized_docs:\n",
        "    for term in tokenized_doc:\n",
        "        token_set.add(term)"
      ],
      "metadata": {
        "id": "39cM3HdzUXq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {word : index for index, word in enumerate(token_set)}\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWtH6htRUwE4",
        "outputId": "702cb27a-4676-487b-d604-bceb0cf30681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'love': 0, 'hate': 1, 'nlp': 2, 'kim': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_vectors = {}\n",
        "for word, index in vocab.items():\n",
        "    one_hot_vector = [0]*(len(vocab))\n",
        "    one_hot_vector[index] = 1\n",
        "    one_hot_vectors[word] = one_hot_vector\n",
        "one_hot_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an2zNGJfWfsq",
        "outputId": "60969560-6ebd-4160-bb3d-a2c1d36d2ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'love': [1, 0, 0, 0],\n",
              " 'hate': [0, 1, 0, 0],\n",
              " 'nlp': [0, 0, 1, 0],\n",
              " 'kim': [0, 0, 0, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-Words"
      ],
      "metadata": {
        "id": "4saeH5fAxVMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_docs = {}\n",
        "for doc_id, tokenized_doc in enumerate(tokenized_docs):\n",
        "    counter = Counter(tokenized_doc)\n",
        "    bow_doc = {}\n",
        "    for word in vocab:\n",
        "        bow_doc[word] = counter.get(word, 0)\n",
        "    bow_docs[doc_id] = bow_doc\n",
        "bow_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcSKFs6uXb6R",
        "outputId": "ce9a9ab6-1c96-4d5b-d2db-285a65a85c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'love': 1, 'hate': 1, 'nlp': 2, 'kim': 2},\n",
              " 1: {'love': 0, 'hate': 1, 'nlp': 1, 'kim': 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVtVC7LnqS9T"
      },
      "source": [
        "## TFIDF (Term Frequency Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSeX6VnhTeA5"
      },
      "source": [
        "**Document Frequency (DF)**\n",
        "\n",
        "- *df(t) = occurrence of t in documents*\n",
        "\n",
        "- idf(t) = log(N/(df + 1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEOTCuiZTdl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3e1930-be24-4ce1-8ceb-9ff9cdbe7db1"
      },
      "source": [
        "dfs = defaultdict(int)\n",
        "for tokenized_doc in tokenized_docs:\n",
        "    # get each unique word in the doc - we need to know whether the word is appeared in the document\n",
        "    for term in np.unique(tokenized_doc):\n",
        "        dfs[term] +=1\n",
        "dfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'hate': 2, 'kim': 2, 'love': 1, 'nlp': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEpq2ws8Lsos"
      },
      "source": [
        "**TF-IDF calculation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNeraCvfIO3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10664a72-964b-4b3f-a8df-ca6ffcbacc3a"
      },
      "source": [
        "N = len(tokenized_docs)\n",
        "\n",
        "tf_idfs = {}\n",
        "for doc_id, tokenized_doc in enumerate(tokenized_docs):\n",
        "    counter = Counter(tokenized_doc)\n",
        "    total_num_words = len(tokenized_doc)\n",
        "    for term in np.unique(tokenized_doc):\n",
        "        tf = counter.get(term, 0)/total_num_words\n",
        "        df = dfs[term]\n",
        "        idf = math.log(N/(df+1))+1 # add 1 not to be negative\n",
        "        tf_idfs[doc_id, term] = round(tf*idf, 3)\n",
        "tf_idfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 'hate'): 0.099,\n",
              " (0, 'kim'): 0.198,\n",
              " (0, 'love'): 0.167,\n",
              " (0, 'nlp'): 0.198,\n",
              " (1, 'hate'): 0.198,\n",
              " (1, 'kim'): 0.198,\n",
              " (1, 'nlp'): 0.198}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tETfBpygMC-u"
      },
      "source": [
        "**Sort by the importance - Descending Order**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R4BI1-UH35-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb485955-1d32-4bc3-aa3f-40bd6225c03e"
      },
      "source": [
        "#sort the dictionary based on values\n",
        "sorted_list = sorted(tf_idfs.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((0, 'kim'), 0.198),\n",
              " ((0, 'nlp'), 0.198),\n",
              " ((1, 'hate'), 0.198),\n",
              " ((1, 'kim'), 0.198),\n",
              " ((1, 'nlp'), 0.198),\n",
              " ((0, 'love'), 0.167),\n",
              " ((0, 'hate'), 0.099)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqD22EW5PZRa"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "**Write a function which returns the top N (e.g. 10 or 20) words with the largest tf value and with the largest tfidf values for a paragraph of corpus (Wikipedia page).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAd2bqFR3CfJ",
        "outputId": "5df5415f-90a3-475e-bfce-2bfecf7fef7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q wikipedia\n",
        "import wikipedia"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JSYTirTx6ML"
      },
      "source": [
        "animals = ['Kangaroo', 'Quokka', 'Capybara']\n",
        "corpus = [wikipedia.page(animal).content for animal in animals]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgdayfSF_RW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c1fc97-dab1-4be0-a5a1-c0301b8193c4"
      },
      "source": [
        "# pass both the corpus and the number of words to focus into the function\n",
        "# In the function, you may:\n",
        "# - process the corpus (e.g. tokenization) - provided\n",
        "# - calculate the tf/tfidf values for each unique words\n",
        "# - sorting the words based on the tf/tfidf values\n",
        "# - print out the top n tf words and tfidf words from the sorted list in parallel for comparison\n",
        "\n",
        "def get_tf_and_idf(corpus, top_n):\n",
        "    # Process the corpus (incl. tokenisation, lower_case, stopword removal)\n",
        "    tokenized_docs = []\n",
        "    for doc in corpus:\n",
        "        puct_removed = re.sub(r'[^\\w\\s]','',doc)\n",
        "        tokenized = word_tokenize(puct_removed)\n",
        "        lowered = [t.lower() for t in tokenized]\n",
        "        lemmatized = [lemmatizer.lemmatize(t) for t in lowered]\n",
        "        tokenized_doc = [w for w in lemmatized if not w in stop_words]\n",
        "        tokenized_docs.append(tokenized_doc)\n",
        "\n",
        "    # Please complete this\n",
        "\n",
        "get_tf_and_idf(corpus, 10)\n",
        "\n",
        "#Expected Outcomes are as follows:"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total docs in corpus: 3\n",
            "\n",
            "Top 10 of tf values:\n",
            "(doc id, word): tf\n",
            "(0, 'kangaroo') 0.051\n",
            "(2, 'capybara') 0.05\n",
            "(1, 'quokkas') 0.03\n",
            "(1, 'island') 0.021\n",
            "(1, 'quokka') 0.021\n",
            "(2, 'male') 0.012\n",
            "(0, 'male') 0.011\n",
            "(1, 'ha') 0.011\n",
            "(1, 'population') 0.011\n",
            "(1, 'rottnest') 0.011\n",
            "\n",
            "Top 10 of tfidf values:\n",
            "(doc id, word): tf*idf\n",
            "(2, 'capybara') 0.07\n",
            "(0, 'kangaroo') 0.051\n",
            "(1, 'quokkas') 0.042\n",
            "(1, 'quokka') 0.03\n",
            "(1, 'island') 0.021\n",
            "(1, 'rottnest') 0.016\n",
            "(1, 'mainland') 0.012\n",
            "(2, 'male') 0.012\n",
            "(0, 'male') 0.011\n",
            "(1, 'small') 0.011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jWcRcHl1PVsc"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}