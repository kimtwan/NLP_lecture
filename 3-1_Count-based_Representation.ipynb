{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1avQbIcRH7i2sxn8KDZH2w7LmU-3zIt_o","timestamp":1697175543082}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3-1. **Count-based Representation**"],"metadata":{"id":"QBX084k_RY2D"}},{"cell_type":"code","source":["!pip install -q nltk"],"metadata":{"id":"XtspuIH4pUx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import math\n","import numpy as np\n","from collections import Counter, defaultdict\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords as sw\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yd5k7yOzFrmc","executionInfo":{"status":"ok","timestamp":1697553071609,"user_tz":-540,"elapsed":1853,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"5a821ddb-d13a-45fa-ae96-493ab128ae86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["corpus = [\n","    'Kim loves the NLP. The NLP hates Kim', # document 0\n","    'Kim hates the NLP' # document 1\n","]"],"metadata":{"id":"iIDiin57TKpg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pre-process the documents"],"metadata":{"id":"b-dWVBvshQdR"}},{"cell_type":"code","metadata":{"id":"hcjJPxPIHsVo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697553073838,"user_tz":-540,"elapsed":2247,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"5520e55c-f354-4c39-f385-e4e0817998ea"},"source":["stop_words = sw.words('english')\n","lemmatizer = WordNetLemmatizer()\n","\n","tokenized_docs = []\n","for doc_id, doc in enumerate(corpus):\n","    print('doc_id:', doc_id)\n","    # remove punctuations\n","    puct_removed = re.sub(r'[^\\w\\s]','',doc)\n","    print('punctuations removed:', puct_removed)\n","    # tokenize words\n","    tokenized = word_tokenize(puct_removed)\n","    print('tokenized:', tokenized)\n","    # case folding\n","    lowered = [t.lower() for t in tokenized]\n","    print('case-folded:', lowered)\n","    # lemmatization\n","    lemmatized = [lemmatizer.lemmatize(t) for t in lowered]\n","    print('lemmatized:', lemmatized)\n","    # stop word removal\n","    tokenized_doc = [w for w in lemmatized if not w in stop_words]\n","    print('stopwords removed:', tokenized_doc)\n","    tokenized_docs.append(tokenized_doc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["doc_id: 0\n","punctuations removed: Kim loves the NLP The NLP hates Kim\n","tokenized: ['Kim', 'loves', 'the', 'NLP', 'The', 'NLP', 'hates', 'Kim']\n","case-folded: ['kim', 'loves', 'the', 'nlp', 'the', 'nlp', 'hates', 'kim']\n","lemmatized: ['kim', 'love', 'the', 'nlp', 'the', 'nlp', 'hate', 'kim']\n","stopwords removed: ['kim', 'love', 'nlp', 'nlp', 'hate', 'kim']\n","doc_id: 1\n","punctuations removed: Kim hates the NLP\n","tokenized: ['Kim', 'hates', 'the', 'NLP']\n","case-folded: ['kim', 'hates', 'the', 'nlp']\n","lemmatized: ['kim', 'hate', 'the', 'nlp']\n","stopwords removed: ['kim', 'hate', 'nlp']\n"]}]},{"cell_type":"code","source":["tokenized_docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDHm9lxmkMeG","executionInfo":{"status":"ok","timestamp":1697553073839,"user_tz":-540,"elapsed":63,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"2a2fbf0b-82c3-447b-db85-faab40021649"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['kim', 'love', 'nlp', 'nlp', 'hate', 'kim'], ['kim', 'hate', 'nlp']]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## One-hot encoding"],"metadata":{"id":"Ih1ngdS8ovvN"}},{"cell_type":"code","source":["token_set = set()\n","for tokenized_doc in tokenized_docs:\n","    for term in tokenized_doc:\n","        token_set.add(term)"],"metadata":{"id":"39cM3HdzUXq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = {word : index for index, word in enumerate(token_set)}\n","vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OWtH6htRUwE4","executionInfo":{"status":"ok","timestamp":1697553073840,"user_tz":-540,"elapsed":54,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"702cb27a-4676-487b-d604-bceb0cf30681"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'love': 0, 'hate': 1, 'nlp': 2, 'kim': 3}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["one_hot_vectors = {}\n","for word, index in vocab.items():\n","    one_hot_vector = [0]*(len(vocab))\n","    one_hot_vector[index] = 1\n","    one_hot_vectors[word] = one_hot_vector\n","one_hot_vectors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"an2zNGJfWfsq","executionInfo":{"status":"ok","timestamp":1697553073841,"user_tz":-540,"elapsed":50,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"60969560-6ebd-4160-bb3d-a2c1d36d2ef8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'love': [1, 0, 0, 0],\n"," 'hate': [0, 1, 0, 0],\n"," 'nlp': [0, 0, 1, 0],\n"," 'kim': [0, 0, 0, 1]}"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## Bag-of-Words"],"metadata":{"id":"4saeH5fAxVMT"}},{"cell_type":"code","source":["bow_docs = {}\n","for doc_id, tokenized_doc in enumerate(tokenized_docs):\n","    counter = Counter(tokenized_doc)\n","    bow_doc = {}\n","    for word in vocab:\n","        bow_doc[word] = counter.get(word, 0)\n","    bow_docs[doc_id] = bow_doc\n","bow_docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcSKFs6uXb6R","executionInfo":{"status":"ok","timestamp":1697553073842,"user_tz":-540,"elapsed":42,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"ce9a9ab6-1c96-4d5b-d2db-285a65a85c1c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: {'love': 1, 'hate': 1, 'nlp': 2, 'kim': 2},\n"," 1: {'love': 0, 'hate': 1, 'nlp': 1, 'kim': 1}}"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"RVtVC7LnqS9T"},"source":["## TFIDF (Term Frequency Inverse Document Frequency)"]},{"cell_type":"markdown","metadata":{"id":"NSeX6VnhTeA5"},"source":["**Document Frequency (DF)**\n","\n","- *df(t) = occurrence of t in documents*\n","\n","- idf(t) = log(N/(df + 1))\n","\n"]},{"cell_type":"code","metadata":{"id":"aEOTCuiZTdl6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697553073843,"user_tz":-540,"elapsed":37,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"5e3e1930-be24-4ce1-8ceb-9ff9cdbe7db1"},"source":["dfs = defaultdict(int)\n","for tokenized_doc in tokenized_docs:\n","    # get each unique word in the doc - we need to know whether the word is appeared in the document\n","    for term in np.unique(tokenized_doc):\n","        dfs[term] +=1\n","dfs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(int, {'hate': 2, 'kim': 2, 'love': 1, 'nlp': 2})"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"AEpq2ws8Lsos"},"source":["**TF-IDF calculation**"]},{"cell_type":"code","metadata":{"id":"tNeraCvfIO3E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697553342396,"user_tz":-540,"elapsed":324,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"10664a72-964b-4b3f-a8df-ca6ffcbacc3a"},"source":["N = len(tokenized_docs)\n","\n","tf_idfs = {}\n","for doc_id, tokenized_doc in enumerate(tokenized_docs):\n","    counter = Counter(tokenized_doc)\n","    total_num_words = len(tokenized_doc)\n","    for term in np.unique(tokenized_doc):\n","        tf = counter.get(term, 0)/total_num_words\n","        df = dfs[term]\n","        idf = math.log(N/(df+1))+1 # add 1 not to be negative\n","        tf_idfs[doc_id, term] = round(tf*idf, 3)\n","tf_idfs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(0, 'hate'): 0.099,\n"," (0, 'kim'): 0.198,\n"," (0, 'love'): 0.167,\n"," (0, 'nlp'): 0.198,\n"," (1, 'hate'): 0.198,\n"," (1, 'kim'): 0.198,\n"," (1, 'nlp'): 0.198}"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"tETfBpygMC-u"},"source":["**Sort by the importance - Descending Order**"]},{"cell_type":"code","metadata":{"id":"0R4BI1-UH35-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697553088623,"user_tz":-540,"elapsed":384,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"fb485955-1d32-4bc3-aa3f-40bd6225c03e"},"source":["#sort the dictionary based on values\n","sorted_list = sorted(tf_idfs.items(), key=lambda x: x[1], reverse=True)\n","sorted_list"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[((0, 'kim'), 0.198),\n"," ((0, 'nlp'), 0.198),\n"," ((1, 'hate'), 0.198),\n"," ((1, 'kim'), 0.198),\n"," ((1, 'nlp'), 0.198),\n"," ((0, 'love'), 0.167),\n"," ((0, 'hate'), 0.099)]"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"aqD22EW5PZRa"},"source":["# Exercise\n","\n","**Write a function which returns the top N (e.g. 10 or 20) words with the largest tf value and with the largest tfidf values for a paragraph of corpus (Wikipedia page).**"]},{"cell_type":"code","metadata":{"id":"bAd2bqFR3CfJ"},"source":["!pip install -q wikipedia\n","import wikipedia"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4JSYTirTx6ML"},"source":["animals = ['Kangaroo', 'Wallaby', 'Quokka', 'Chipmunk', 'Rabbit']\n","corpus = [wikipedia.page(animal).content for animal in animals]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgdayfSF_RW-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697469074100,"user_tz":-540,"elapsed":539,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"046bd0ce-5cd8-40e3-bd81-513c1937b81b"},"source":["# pass both the corpus and the number of words to focus into the function\n","# In the function, you may:\n","# - process the corpus (e.g. tokenization) - provided\n","# - calculate the tf/tfidf values for each unique words\n","# - sorting the words based on the tf/tfidf values\n","# - print out the top n tf words and tfidf words from the sorted list in parallel for comparison\n","\n","def get_tf_and_idf(corpus, top_n):\n","    # Process the corpus (incl. tokenisation, lower_case, stopword removal)\n","    tokenized_docs = []\n","    for doc in corpus:\n","        puct_removed = re.sub(r'[^\\w\\s]','',doc)\n","        tokenized = word_tokenize(puct_removed)\n","        lowered = [t.lower() for t in tokenized]\n","        lemmatized = [lemmatizer.lemmatize(t) for t in lowered]\n","        tokenized_doc = [w for w in lemmatized if not w in stop_words]\n","        tokenized_docs.append(tokenized_doc)\n","\n","    # Please complete this\n","\n","get_tf_and_idf(corpus, 10)\n","\n","#Expected Outcomes are as follows:"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total docs in corpus: 5\n","\n","Top 10 of tf values:\n","(doc id, word): tf\n","(3, 'chipmunk') 0.09\n","(4, 'rabbit') 0.067\n","(1, 'wallaby') 0.059\n","(0, 'kangaroo') 0.049\n","(3, 'neotamias') 0.043\n","(2, 'quokkas') 0.028\n","(2, 'quokka') 0.021\n","(2, 'island') 0.02\n","(1, 'petrogale') 0.019\n","(1, 'specie') 0.019\n","\n","Top 10 of tfidf values:\n","(doc id, word): tf*idf\n","(3, 'chipmunk') 0.173\n","(4, 'rabbit') 0.129\n","(3, 'neotamias') 0.083\n","(1, 'wallaby') 0.072\n","(0, 'kangaroo') 0.06\n","(2, 'quokkas') 0.053\n","(1, 'petrogale') 0.037\n","(2, 'quokka') 0.032\n","(1, 'rockwallaby') 0.03\n","(1, 'notamacropus') 0.027\n"]}]}]}