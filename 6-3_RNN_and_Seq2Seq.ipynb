{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1K7Oqoh1-BnTRVrquy1iFQqhefD0HlrVP","timestamp":1664983677983},{"file_id":"1VU8sYHb1oK7UYOz7jRj-ZcKJsodOdJuL","timestamp":1585021213921},{"file_id":"1OgM-PGUzFeAUsvjkq4BZJl5xJnmJsPaj","timestamp":1584997583700},{"file_id":"1_gKHGIuCsuYZUGmhZDblk-lSNgSy_ifh","timestamp":1584847615260},{"file_id":"1DKOAFNCATByf8gDqzhum0IvvIuLHDs7H","timestamp":1584743941972}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JgAANQLNyE3p"},"source":["# 6-3. **RNN and Seq2Seq**\n","In this lab, we learn the Recurrent Neural Networks and Sequence Modelling\n","\n","*   Recurrent Neural Networks\n","*   Sequence Modelling (Seq2Seq)\n"]},{"cell_type":"code","metadata":{"id":"fTls5pyuluqb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697640857954,"user_tz":-540,"elapsed":3124,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"f91d0813-5442-4b25-de6d-293842f71a0a"},"source":["import torch\n","# You can enable GPU here (cuda); or just CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"0vysAG7fyIQJ"},"source":["## RNN\n","A **recurrent neural network (RNN)** is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs."]},{"cell_type":"markdown","metadata":{"id":"HEqu0hg8DaEh"},"source":["\n","### Predict the last character of the word"]},{"cell_type":"code","metadata":{"id":"uw5s2SVGU9qI"},"source":["import numpy as np\n","\n","# Assume that we have the following character instances\n","char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n","            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n","            'o', 'p', 'q', 'r', 's', 't', 'u',\n","            'v', 'w', 'x', 'y', 'z']\n","\n","# for one-hot encoding and decoding\n","# {'a': 0, 'b': 1, 'c': 2, ..., 'j': 9, 'k':10, ...}\n","num_dic = {n: i for i, n in enumerate(char_arr)}\n","dic_len = len(num_dic)\n","\n","# a list words for sequence data (input and output)\n","seq_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n","\n","# Make a batch to have sequence data for input and ouput\n","# wor -> X, d -> Y\n","# dee -> X, p -> Y\n","def make_batch(seq_data):\n","    input_batch = []\n","    target_batch = []\n","\n","    for seq in seq_data:\n","        # input data is:\n","        #     wor           woo        dee       div\n","        # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n","        input_data = [num_dic[n] for n in seq[:-1]]\n","\n","        # target is :\n","        # d, d, p, e, ...\n","        # 3, 3, 15, 4, ...\n","        target = num_dic[seq[-1]]\n","\n","        # convert input to one-hot encoding.\n","        # if input is [3, 4, 4]:\n","        # [[ 0,  0,  0,  1,  0,  0,  0, ... 0]\n","        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]\n","        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]]\n","        input_batch.append(np.eye(dic_len)[input_data])\n","\n","        target_batch.append([target])\n","\n","    return input_batch, target_batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hp5TkPM1gxa"},"source":["### Setting hyperparameters\n","learning_rate = 0.1\n","n_hidden = 128\n","total_epoch = 50\n","\n","# Number of sequences for RNN\n","n_step = 3\n","\n","# number of inputs (dimension of input vector) = 26\n","n_input = dic_len\n","# number of classes = 26\n","n_class = dic_len"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0NPv3hP7Oou"},"source":["### Dropout\n","\n","Dropout makes each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes\n","\n","![dropout](https://cdn-images-1.medium.com/max/800/1*D8jriroKkjno8RztHKmMnA.png)"]},{"cell_type":"markdown","metadata":{"id":"TW_Usnk1OBFP"},"source":["### Model"]},{"cell_type":"code","metadata":{"id":"RQzn33GGgyuk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697640871151,"user_tz":-540,"elapsed":13202,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"c7d64f02-256b-444a-a863-23f86e604eec"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # LSTM layer, the batch_first is False by default, which means the input and output tensors are provided as (seq_len, batch_size, feature)\n","        # We need to set it to True because we are using input of shape (batch_size, seq_len, feature)\n","        # Apply dropout to prevent overfitting, you can try to change the dropout rate. Note that this dropout is applied on outputs of each LSTM layer except the last layer\n","        self.lstm = nn.LSTM(n_input, n_hidden, num_layers=2, batch_first=True, dropout=0.2)\n","        # Linear layer for output\n","        self.linear = nn.Linear(n_hidden, n_class)\n","\n","    def forward(self, x):\n","        # There are two outputs from nn.LSTM:\n","        # 1. tensor of shape (batch_size, seq_len, hidden_size) containing the output features from the last layer of the LSTM for each time step t\n","        # 2. the tuple containing the hidden state and cell state.\n","        # Here we only care about the first output. Details for the two outputs can be found in PyTorch documentation for nn.LSTM: https://pytorch.org/docs/stable/nn.html#lstm\n","        x, _ = self.lstm(x)\n","        # Here we extract only the last hidden state from the LSTM output features\n","        # The last hidden carries the information about what the LSTM cell has seen over the time.\n","        # Thus the prediction based on the last hidden state not only considers the data at the current time step, instead, it considers historical data.\n","        x = self.linear(x[:, -1, :])\n","        x = F.log_softmax(x, dim=1)\n","        return x\n","\n","# Move the model to GPU\n","net = Net().to(device)\n","# Loss function and optimizer\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","# Preparing input\n","input_batch, target_batch = make_batch(seq_data)\n","# Convert input into tensors and move them to GPU by uting tensor.to(device)\n","input_batch_torch = torch.from_numpy(np.array(input_batch)).float().to(device)\n","target_batch_torch = torch.from_numpy(np.array(target_batch)).view(-1).to(device) # flatten the tensor\n","\n","for epoch in range(total_epoch):\n","    # Set the flag to training\n","    net.train()\n","\n","    # forward + backward + optimize\n","    outputs = net(input_batch_torch)\n","    loss = criterion(outputs, target_batch_torch)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # Set the flag to evaluation, which will 'turn off' the dropout\n","    net.eval()\n","    outputs = net(input_batch_torch)\n","\n","    # Evaluation loss and accuracy calculation\n","    loss = criterion(outputs, target_batch_torch)\n","    _, predicted = torch.max(outputs, 1)\n","    acc= accuracy_score(predicted.cpu().numpy(), target_batch_torch.cpu().numpy())\n","\n","    print('Epoch: %d, loss: %.5f, train_acc: %.2f' %(epoch + 1, loss.item(), acc))\n","\n","print('Finished Training')\n","\n","## Prediction\n","predict_words = []\n","for i in range(len(predicted.cpu().numpy())):\n","    ind = predicted.cpu().numpy()[i]\n","    predict_words.append(seq_data[i][:-1]+char_arr[ind])\n","\n","print('\\n=== Prediction Result ===')\n","print('Input:', [w[:3] + ' ' for w in seq_data])\n","print('Predicted:', predict_words)\n","print('Accuracy: %.2f' %acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, loss: 1.43037, train_acc: 0.50\n","Epoch: 2, loss: 3.53669, train_acc: 0.20\n","Epoch: 3, loss: 8.63431, train_acc: 0.00\n","Epoch: 4, loss: 1.96001, train_acc: 0.10\n","Epoch: 5, loss: 1.46228, train_acc: 0.50\n","Epoch: 6, loss: 1.55171, train_acc: 0.50\n","Epoch: 7, loss: 1.24376, train_acc: 0.50\n","Epoch: 8, loss: 1.13213, train_acc: 0.50\n","Epoch: 9, loss: 1.25247, train_acc: 0.30\n","Epoch: 10, loss: 1.15907, train_acc: 0.50\n","Epoch: 11, loss: 1.09400, train_acc: 0.50\n","Epoch: 12, loss: 1.18635, train_acc: 0.50\n","Epoch: 13, loss: 1.14358, train_acc: 0.50\n","Epoch: 14, loss: 1.06540, train_acc: 0.50\n","Epoch: 15, loss: 1.06323, train_acc: 0.60\n","Epoch: 16, loss: 1.09642, train_acc: 0.70\n","Epoch: 17, loss: 1.07642, train_acc: 0.50\n","Epoch: 18, loss: 1.02311, train_acc: 0.70\n","Epoch: 19, loss: 1.00924, train_acc: 0.70\n","Epoch: 20, loss: 0.97392, train_acc: 0.70\n","Epoch: 21, loss: 0.93634, train_acc: 0.70\n","Epoch: 22, loss: 0.90148, train_acc: 0.70\n","Epoch: 23, loss: 0.94756, train_acc: 0.60\n","Epoch: 24, loss: 1.00739, train_acc: 0.60\n","Epoch: 25, loss: 1.01667, train_acc: 0.60\n","Epoch: 26, loss: 0.90810, train_acc: 0.60\n","Epoch: 27, loss: 0.91877, train_acc: 0.60\n","Epoch: 28, loss: 0.98288, train_acc: 0.50\n","Epoch: 29, loss: 0.92586, train_acc: 0.60\n","Epoch: 30, loss: 0.87447, train_acc: 0.60\n","Epoch: 31, loss: 0.82712, train_acc: 0.70\n","Epoch: 32, loss: 0.74225, train_acc: 0.70\n","Epoch: 33, loss: 0.74764, train_acc: 0.70\n","Epoch: 34, loss: 0.75296, train_acc: 0.70\n","Epoch: 35, loss: 0.73829, train_acc: 0.70\n","Epoch: 36, loss: 0.74825, train_acc: 0.70\n","Epoch: 37, loss: 0.74745, train_acc: 0.50\n","Epoch: 38, loss: 0.72436, train_acc: 0.70\n","Epoch: 39, loss: 0.73814, train_acc: 0.70\n","Epoch: 40, loss: 0.69756, train_acc: 0.70\n","Epoch: 41, loss: 0.67745, train_acc: 0.70\n","Epoch: 42, loss: 0.70124, train_acc: 0.60\n","Epoch: 43, loss: 0.66920, train_acc: 0.70\n","Epoch: 44, loss: 0.72591, train_acc: 0.70\n","Epoch: 45, loss: 0.72363, train_acc: 0.70\n","Epoch: 46, loss: 0.66460, train_acc: 0.70\n","Epoch: 47, loss: 0.62330, train_acc: 0.70\n","Epoch: 48, loss: 0.60821, train_acc: 0.70\n","Epoch: 49, loss: 0.59856, train_acc: 0.70\n","Epoch: 50, loss: 0.57573, train_acc: 0.70\n","Finished Training\n","\n","=== Prediction Result ===\n","Input: ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n","Predicted: ['word', 'wood', 'deep', 'dive', 'coll', 'cool', 'load', 'lovd', 'kisd', 'kind']\n","Accuracy: 0.70\n"]}]},{"cell_type":"markdown","metadata":{"id":"XfRXWwL2D9Bn"},"source":["## Seq2Seq Model (N to M)"]},{"cell_type":"markdown","metadata":{"id":"UFRHe5J9c3QG"},"source":["Seq2seq turns one sequence into another sequence. It does so by use of a recurrent neural network (RNN) or more often LSTM or GRU to avoid the problem of vanishing gradient. The context for each item is the output from the previous step. The primary components are one encoder and one decoder network. The encoder turns each item into a corresponding hidden vector containing the item and its context. The decoder reverses the process, turning the vector into an output item, using the previous output as the input context\n","\n","We are going to implement a sequence to sequence model that translates playing card symbols (Ace, Jack, Queen, King) to their associated number."]},{"cell_type":"markdown","metadata":{"id":"1yEhcJZxxWvu"},"source":["### Preprocess data"]},{"cell_type":"code","metadata":{"id":"MyTFAjAYlIgn"},"source":["import torch\n","import numpy as np\n","\n","# Sequence data\n","seq_data = [['ace', '01'], ['jack', '11'],\n","            ['queen', '12'], ['king', '13']]\n","\n","# Generate unique tokens list\n","chars = []\n","for seq in seq_data:\n","    chars += list(seq[0])\n","    chars += list(seq[1])\n","\n","char_arr = list(set(chars))\n","\n","# special tokens are required\n","# B: Beginning of Sequence\n","# E: Ending of Sequence\n","# P: Padding of Sequence - for different size input\n","# U: Unknown element of Sequence - for different size input\n","char_arr.append('B')\n","char_arr.append('E')\n","char_arr.append('P')\n","char_arr.append('U')\n","\n","num_dic = {n: i for i, n in enumerate(char_arr)}\n","\n","dic_len = len(num_dic)\n","\n","max_input_words_amount = 5\n","max_output_words_amount = 3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gKk4ORyCxbjE"},"source":["### Generate batch"]},{"cell_type":"code","metadata":{"id":"UNPoWhYDIL71"},"source":["# add paddings if the word is shorter than the maximum number of words\n","def add_paddings(word):\n","    diff = 5 - len(word)\n","    for x in range(diff):\n","        word += 'P'\n","    return word\n","\n","# generate a batch data for training/testing\n","def make_batch(seq_data):\n","    input_batch = []\n","    output_batch = []\n","    target_batch = []\n","\n","    for seq in seq_data:\n","        # Input for encoder cell, convert to vector\n","        input_word = add_paddings(seq[0])\n","        input_data = [num_dic[n] for n in input_word]\n","\n","        # Input for decoder cell, Add 'B' at the beginning of the sequence data\n","        output_data  = [num_dic[n] for n in ('B'+ seq[1])]\n","\n","        # Output of decoder cell (Actual result), Add 'E' at the end of the sequence data\n","        target = [num_dic[n] for n in (seq[1] + 'E')]\n","\n","        # Convert each character vector to one-hot encode data\n","        input_batch.append(np.eye(dic_len)[input_data])\n","        output_batch.append(np.eye(dic_len)[output_data])\n","\n","        target_batch.append(target)\n","\n","    return input_batch, output_batch, target_batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"US15fGgPxgBf"},"source":["### Build training model"]},{"cell_type":"code","metadata":{"id":"sSq1w6UY94cE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697640872240,"user_tz":-540,"elapsed":1093,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"f10e26ed-a5f7-40d8-8d45-ed9d64ed8449"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # RNN encoder. The parameters of nn.RNN is similar to nn.LSTM\n","        self.rnn_encoder = nn.RNN(n_input, n_hidden, batch_first=True)\n","        # Apply the drop out to output of RNN. Note the difference here compared to the 'dropout=0.2' we used for nn.LSTM above\n","        self.dropout_encoder = nn.Dropout(0.1)\n","\n","        # RNN decoder\n","        self.rnn_decoder = nn.RNN(n_input, n_hidden, batch_first=True)\n","        self.dropout_decoder = nn.Dropout(0.1)\n","        self.linear = nn.Linear(n_hidden, n_class)\n","\n","    def forward(self, x_encoder, x_decoder):\n","        # 'hidden' containing the hidden state for t=seq_len.\n","        _, hidden = self.rnn_encoder(x_encoder)\n","        hidden = self.dropout_encoder(hidden)\n","        # [IMPORTANT] Setting 'hidden' as inital_state of rnn_decoder!!\n","        decoder_output, _ = self.rnn_decoder(x_decoder, hidden)\n","        decoder_output = self.dropout_decoder(decoder_output)\n","        prediction_output_before_softmax = self.linear(decoder_output)\n","        output = torch.log_softmax(prediction_output_before_softmax, dim=-1)\n","        return output\n","\n","### Setting Hyperparameters\n","learning_rate = 0.01\n","n_hidden = 128\n","total_epoch = 200\n","\n","n_class = dic_len\n","n_input = dic_len\n","\n","net = Net().to(device)\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n","\n","input_batch, output_batch, target_batch = make_batch(seq_data)\n","input_batch_torch = torch.from_numpy(np.array(input_batch)).float().to(device)\n","output_batch_torch = torch.from_numpy(np.array(output_batch)).float().to(device)\n","target_batch_torch = torch.from_numpy(np.array(target_batch)).view(-1).to(device)\n","\n","for epoch in range(total_epoch): # loop over the dataset multiple times\n","    net.train()\n","    optimizer.zero_grad()\n","\n","    # forward + backward + optimize\n","    outputs = net(input_batch_torch, output_batch_torch)\n","    loss = criterion(outputs.view(-1, outputs.size(-1)), target_batch_torch)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch % 10==9:\n","        print('Epoch: %d, loss: %.5f' %(epoch + 1, loss.item()))\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 10, loss: 0.48664\n","Epoch: 20, loss: 0.46422\n","Epoch: 30, loss: 0.22405\n","Epoch: 40, loss: 0.19981\n","Epoch: 50, loss: 0.10271\n","Epoch: 60, loss: 0.16692\n","Epoch: 70, loss: 0.75095\n","Epoch: 80, loss: 0.26325\n","Epoch: 90, loss: 0.49039\n","Epoch: 100, loss: 0.25200\n","Epoch: 110, loss: 0.26218\n","Epoch: 120, loss: 0.26386\n","Epoch: 130, loss: 0.40053\n","Epoch: 140, loss: 0.29064\n","Epoch: 150, loss: 0.24221\n","Epoch: 160, loss: 0.23084\n","Epoch: 170, loss: 0.13081\n","Epoch: 180, loss: 0.10123\n","Epoch: 190, loss: 0.06774\n","Epoch: 200, loss: 0.02969\n","Finished Training\n"]}]},{"cell_type":"markdown","metadata":{"id":"bXf-4bByxgh8"},"source":["### Evaluation"]},{"cell_type":"code","metadata":{"id":"bTZrBs5PmYGH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697640872241,"user_tz":-540,"elapsed":5,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"de06131c-61f2-4533-b74a-0a6da3b01330"},"source":["def predict(word):\n","    # Setting each character of predicted as 'U' (Unknown)\n","    # ['king', 'UU']\n","    word = add_paddings(word)\n","    seq_data = [word, 'U' * 2]\n","\n","    input_batch, output_batch, target_batch = make_batch([seq_data])\n","    input_batch_torch = torch.from_numpy(np.array(input_batch)).float().to(device)\n","    output_batch_torch = torch.from_numpy(np.array(output_batch)).float().to(device)\n","\n","    # forward + backward + optimize\n","    net.eval()\n","    outputs = net(input_batch_torch, output_batch_torch)\n","    _, predicted = torch.max(outputs, -1)\n","\n","    answer = ''\n","    for i in range(len(predicted.cpu().numpy()[0]) - 1):\n","        answer += char_arr[predicted.cpu().numpy()[0][i]]\n","    return answer\n","\n","print('=== Prediction result ===')\n","print('ace ->', predict('ace'))\n","print('jack ->', predict('jack'))\n","print('queen ->', predict('queen'))\n","print('king ->', predict('king'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Prediction result ===\n","ace -> 01\n","jack -> 11\n","queen -> 12\n","king -> 13\n"]}]},{"cell_type":"markdown","metadata":{"id":"M-316qjIt-zS"},"source":["## Exercise (Text classification using LSTM)"]},{"cell_type":"markdown","metadata":{"id":"Jwj7GtQPP6M7"},"source":["In this exercise, you are going to implement a LSTM model to do the text classification problem. Please notice that we have already done the preprocessing and embedding part of the dataset. You can only focus on the Model part.\n","\n","**Sequence Modelling**\n","\n","![alt text](https://usydnlpgroup.files.wordpress.com/2020/03/lstm_textclassification-e1584855309361.png)\n","\n"]},{"cell_type":"code","metadata":{"id":"h6E9-9EqTNns"},"source":["import torch\n","# If you enable GPU here, device will be cuda, otherwise it will be cpu\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X-hlDQBqsV1L"},"source":["### Downloading dataset"]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/kimtwan/NLP_lecture/master/data/embedding_part.zip\n","!wget https://raw.githubusercontent.com/kimtwan/NLP_lecture/master/data/embedding_part.z01\n","!zip -F embedding_part.zip --out embedding.zip\n","!unzip embedding.zip"],"metadata":{"id":"DJaOsWq7-ZVb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697640891937,"user_tz":-540,"elapsed":3071,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"1fbd2493-ff9a-4968-d1dd-b41a93820272"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-18 14:54:48--  https://raw.githubusercontent.com/kimtwan/NLP_lecture/master/data/embedding_part.zip\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15408244 (15M) [application/zip]\n","Saving to: ‘embedding_part.zip’\n","\n","embedding_part.zip  100%[===================>]  14.69M  --.-KB/s    in 0.1s    \n","\n","2023-10-18 14:54:49 (128 MB/s) - ‘embedding_part.zip’ saved [15408244/15408244]\n","\n","--2023-10-18 14:54:49--  https://raw.githubusercontent.com/kimtwan/NLP_lecture/master/data/embedding_part.z01\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20971520 (20M) [application/octet-stream]\n","Saving to: ‘embedding_part.z01’\n","\n","embedding_part.z01  100%[===================>]  20.00M  --.-KB/s    in 0.1s    \n","\n","2023-10-18 14:54:50 (156 MB/s) - ‘embedding_part.z01’ saved [20971520/20971520]\n","\n","Fix archive (-F) - assume mostly intact archive\n"," copying: train.pkl\n"," copying: label.pkl\n","Archive:  embedding.zip\n","  inflating: train.pkl               \n","  inflating: label.pkl               \n"]}]},{"cell_type":"code","metadata":{"id":"jvwyvmfDegwE"},"source":["import pickle\n","input_embeddings = pickle.load(open('train.pkl','rb'))\n","label = pickle.load(open('label.pkl','rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(input_embeddings.shape)\n","print(label.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TydYCZ89TuWP","executionInfo":{"status":"ok","timestamp":1697641362054,"user_tz":-540,"elapsed":11,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"ff21c248-7df9-4d8a-bf6c-f2df0ad6c968"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2257, 512, 25)\n","(2257,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"f4iYaoduQvA9"},"source":["### Split the dataset"]},{"cell_type":"code","metadata":{"id":"XmkbXPApluQq"},"source":["# Split into training and testing dataset using scikit-learn\n","# For more details, you can refer to: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","\n","from sklearn.model_selection import train_test_split\n","train_embeddings, test_embeddings, train_label, test_label = train_test_split(input_embeddings, label, test_size=0.2, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrAL53HKufcG"},"source":["### Generate batch"]},{"cell_type":"code","metadata":{"id":"n4vtcmaZufk2"},"source":["def generate_batch(input_embeddings, label, batch_size):\n","    idx = np.random.randint(input_embeddings.shape[0], size=batch_size)\n","    return input_embeddings[idx, :, :], label[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r_W08vIPv7L7"},"source":["### Model (please complete the following sections)"]},{"cell_type":"markdown","metadata":{"id":"X3rPdMUDQFOU"},"source":["**NOTE**: By updating hyperparameters, you should achieve **at least 0.4** for testset 'weighted avg' f1. (There will be randomness in the training process, so tutors would run your code several times and there should be at least one of the output reaching 0.4)\n","\n","***What is F1?***\n","\n","![alt text](https://1.bp.blogspot.com/-nkFFqViboVM/XWwaQ5x1YpI/AAAAAAAAAP8/XzTH9hfJSfswcRjxSeQFEU6-yKQCwc0EQCLcBGAs/s640/main-qimg-447d6cdb02d2cc097ff1e6083a6bdc37.png)\n","![alt text](https://i.stack.imgur.com/U0hjG.png)\n"]},{"cell_type":"code","metadata":{"id":"fbcKiY-MAbb4"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.lstm = nn.LSTM(n_input, n_hidden, batch_first =True)\n","        self.linear = nn.Linear(n_hidden, n_class)\n","\n","    def forward(self, x):\n","        # Please complete the code for forward propagation\n","        # lstm layer\n","        # linear layer\n","        # softmax layer\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ype-DASTAbb5"},"source":["import numpy as np\n","import torch.optim as optim\n","\n","# Please assign values to these variables by using other variables (instead of hard code)\n","seq_length =\n","n_input =\n","n_class =\n","\n","# Please decide the hyperparameters here by yourself\n","n_hidden = 128\n","batch_size = 64\n","total_epoch = 1000\n","learning_rate = 0.01\n","shown_interval = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKHX_SLVAbb6"},"source":["from sklearn.metrics import accuracy_score\n","\n","net = Net().to(device)\n","criterion = nn.NLLLoss()\n","\n","# Please find which optimizer provide higher f1\n","optimizer =\n","\n","for epoch in range(total_epoch):\n","\n","    input_batch, target_batch = generate_batch(train_embeddings,train_label, batch_size)\n","    input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n","    target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n","\n","    net.train()\n","    outputs = net(input_batch_torch)\n","    loss = criterion(outputs, target_batch_torch)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    if epoch % shown_interval == shown_interval-1:\n","        net.eval()\n","        outputs = net(input_batch_torch)\n","        train_loss = criterion(outputs, target_batch_torch)\n","        _, predicted = torch.max(outputs, 1)\n","        train_acc= accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n","\n","        print('Epoch: %d, train loss: %.5f, train_acc: %.4f'%(epoch + 1, train_loss.item(), train_acc))\n","\n","print('Finished Training')\n","\n","## Prediction\n","net.eval()\n","outputs = net(torch.from_numpy(test_embeddings).float().to(device))\n","_, predicted = torch.max(outputs, 1)\n","\n","from sklearn.metrics import classification_report\n","print(classification_report(test_label, predicted.cpu().numpy(),digits=4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNKfUFdRnQJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697536098294,"user_tz":-540,"elapsed":12057,"user":{"displayName":"Taewan Kim","userId":"13184421950357533683"}},"outputId":"731039d4-bca7-422d-96ce-ac6f441d025a"},"source":["# The following is the sample output\n","# As mentioned in the previous labs, it is impossible to get the same result (randomness in the training process)."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 100, train loss: 1.02840, train_acc: 0.5938\n","Epoch: 200, train loss: 1.33560, train_acc: 0.2812\n","Epoch: 300, train loss: 1.37011, train_acc: 0.3438\n","Epoch: 400, train loss: 1.36462, train_acc: 0.2344\n","Epoch: 500, train loss: 1.36289, train_acc: 0.3125\n","Epoch: 600, train loss: 1.26099, train_acc: 0.3594\n","Epoch: 700, train loss: 0.79372, train_acc: 0.5781\n","Epoch: 800, train loss: 0.70163, train_acc: 0.7031\n","Epoch: 900, train loss: 0.41623, train_acc: 0.7500\n","Epoch: 1000, train loss: 0.33264, train_acc: 0.8906\n","Finished Training\n","              precision    recall  f1-score   support\n","\n","           0     0.4762    0.2151    0.2963        93\n","           1     0.8898    0.9262    0.9076       122\n","           2     0.8692    0.8532    0.8611       109\n","           3     0.6080    0.8359    0.7039       128\n","\n","    accuracy                         0.7367       452\n","   macro avg     0.7108    0.7076    0.6922       452\n","weighted avg     0.7199    0.7367    0.7129       452\n","\n"]}]}]}